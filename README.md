# Neural Networks on Sarcasm Detection  

## Overview
This repository is for comparative study on neural networks purpose and  contains the implementation and process of fine-tuning the BERT model for classification tasks. BERT (Bidirectional Encoder Representations from Transformers) is a powerful pre-trained language model that can be fine-tuned for various natural language processing (NLP) tasks, including text classification.

## Contents
1. **BERT Implementation**: This directory contains the implementation of BERT for classification tasks. It includes the necessary code to preprocess data, fine-tune the BERT model, and evaluate its performance on classification tasks.

2. **Process of Fine-Tuning BERT**: Detailed documentation on the process of fine-tuning BERT for classification tasks. This includes step-by-step instructions, explanations of hyperparameters, and best practices for achieving optimal results.

3. **References**: Links to external resources and references used during the implementation and fine-tuning process. This includes the original BERT repository by codertimo ([GitHub Link](https://github.com/codertimo/BERT-pytorch/tree/master)).

4. **Presentation Recording** [Youtube ðŸ¥°](https://youtu.be/-tHKcdSCcAY)

5. **PowerPoint**: PowerPoint presentation slides used during the presentation recording.

## Contributors
- [Yuqing Qiao]
- [Jingjing Lin]
- [Zhenyu Wang]

## License
This project following Apache 2.0 License as written in LICENSE file
